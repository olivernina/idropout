../data/cifar-100-fixed/batches.meta
../data/cifar-100-fixed/data_batch_2
../data/cifar-100-fixed/batches.meta
../data/cifar-100-fixed/data_batch_1
Initialized data layer 'data', producing 3072 outputs
Initialized data layer 'labels', producing 1 outputs
Initialized convolutional layer 'conv1', producing 32x32 32-channel output
Initialized max-pooling layer 'pool1', producing 16x16 32-channel output
Initialized convolutional layer 'conv2', producing 16x16 32-channel output
Initialized avg-pooling layer 'pool2', producing 8x8 32-channel output
Initialized convolutional layer 'conv3', producing 8x8 64-channel output
Initialized avg-pooling layer 'pool3', producing 4x4 64-channel output
Initialized fully-connected layer 'fc128', producing 128 outputs
Output Drop rate:  0.5
Initialized fully-connected layer 'fc10', producing 100 outputs
Initialized softmax layer 'probs', producing 100 outputs
Initialized logistic regression cost 'logprob'
Initialized neuron layer 'pool1_neuron', producing 8192 outputs
Initialized neuron layer 'conv2_neuron', producing 8192 outputs
Initialized neuron layer 'conv3_neuron', producing 4096 outputs
Initialized neuron layer 'fc128_neuron', producing 128 outputs
=========================
Importing _ConvNet C++ module
============================================
learning rate scale     :  1
Reset Momentum          :  False
Image Rotation & Scaling:  False
============================================
=========================
Training ConvNet
Adaptive Drop Training                              : False [DEFAULT]
Check gradients and quit?                           : 0     [DEFAULT]
Compress checkpoints?                               : 0     [DEFAULT]
Conserve GPU memory (slower)?                       : 1     [DEFAULT]
Convert given conv layers to unshared local         :       
Cropped DP: crop border size                        : 4     [DEFAULT]
Cropped DP: logreg layer name (for --multiview-test):       [DEFAULT]
Cropped DP: test on multiple patches?               : 0     [DEFAULT]
Data batch range: testing                           : 2-2   
Data batch range: training                          : 1-1   
Data path                                           : ../data/cifar-100-fixed/ 
Data provider                                       : cifar 
Enable image rotation and scaling transformation    : False [DEFAULT]
GPU override                                        : -1    [DEFAULT]
Image Size                                          : 0     [DEFAULT]
Layer definition file                               : ./layers-80sec-cifar100.cfg 
Layer parameter file                                : ./layer-params-80sec-cifar100.cfg 
Learning Rate Scale Factor                          : 1     [DEFAULT]
Load file                                           :       [DEFAULT]
Maximum save file size (MB)                         : 0     [DEFAULT]
Minibatch size                                      : 128   [DEFAULT]
Model File Name                                     :       [DEFAULT]
Number of GPUs                                      : 1     [DEFAULT]
Number of channels in image                         : 3     [DEFAULT]
Number of epochs                                    : 600   
Reset layer momentum                                : False [DEFAULT]
Save path                                           : ./    
Test and quit?                                      : 0     [DEFAULT]
Test on one batch at a time?                        : 1     [DEFAULT]
Testing frequency                                   : 15    
Unshare weight matrices in given layers             :       
Whether filp training image                         : True  [DEFAULT]
=========================
Running on CUDA device(s) -2
Current time: Thu Nov  6 13:22:19 2014
Saving checkpoints to ./ConvNet__2014-11-06_13.22.05
=========================
1.1... logprob:  4.526349, 0.979580 (7.231 sec)
epoch_cost: 0
2.1... logprob:  4.361186, 0.964140 (6.663 sec)
epoch_cost: 0
3.1... logprob:  4.133663, 0.928460 (6.655 sec)
epoch_cost: 0
4.1... logprob:  3.913997, 0.892620 (6.678 sec)
epoch_cost: 0
5.1... logprob:  3.736679, 0.864440 (6.625 sec)
epoch_cost: 0
6.1... logprob:  3.571295, 0.836180 (6.655 sec)
epoch_cost: 0
7.1... logprob:  3.446706, 0.814440 (6.723 sec)
epoch_cost: 0
8.1... logprob:  3.338077, 0.792880 (6.698 sec)
epoch_cost: 0
9.1...