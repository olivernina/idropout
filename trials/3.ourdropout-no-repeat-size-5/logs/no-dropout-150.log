Initialized data layer 'data', producing 3072 outputs
Initialized data layer 'labels', producing 1 outputs
Initialized convolutional layer 'conv1', producing 32x32 32-channel output
Initialized max-pooling layer 'pool1', producing 16x16 32-channel output
Initialized convolutional layer 'conv2', producing 16x16 32-channel output
Initialized avg-pooling layer 'pool2', producing 8x8 32-channel output
Initialized convolutional layer 'conv3', producing 8x8 64-channel output
Initialized avg-pooling layer 'pool3', producing 4x4 64-channel output
Initialized fully-connected layer 'fc128', producing 128 outputs
Output Drop rate:  0.5
Initialized fully-connected layer 'fc10', producing 10 outputs
Initialized softmax layer 'probs', producing 10 outputs
Initialized logistic regression cost 'logprob'
Initialized neuron layer 'pool1_neuron', producing 8192 outputs
Initialized neuron layer 'conv2_neuron', producing 8192 outputs
Initialized neuron layer 'conv3_neuron', producing 4096 outputs
Initialized neuron layer 'fc128_neuron', producing 128 outputs
=========================
Importing _ConvNet C++ module
============================================
learning rate scale     :  1
Reset Momentum          :  False
Image Rotation & Scaling:  False
============================================
=========================
Training ConvNet
Adaptive Drop Training                              : False [DEFAULT]
Check gradients and quit?                           : 0     [DEFAULT]
Compress checkpoints?                               : 0     [DEFAULT]
Conserve GPU memory (slower)?                       : 1     [DEFAULT]
Convert given conv layers to unshared local         :       
Cropped DP: crop border size                        : 4     [DEFAULT]
Cropped DP: logreg layer name (for --multiview-test):       [DEFAULT]
Cropped DP: test on multiple patches?               : 0     [DEFAULT]
Data batch range: testing                           : 6-6   
Data batch range: training                          : 1-5   
Data path                                           : ../data/cifar-10/ 
Data provider                                       : cifar 
Enable image rotation and scaling transformation    : False [DEFAULT]
GPU override                                        : -1    [DEFAULT]
Image Size                                          : 0     [DEFAULT]
Layer definition file                               : ./layers-80sec.cfg 
Layer parameter file                                : ./layer-params-80sec.cfg 
Learning Rate Scale Factor                          : 1     [DEFAULT]
Load file                                           :       [DEFAULT]
Maximum save file size (MB)                         : 0     [DEFAULT]
Minibatch size                                      : 128   [DEFAULT]
Model File Name                                     :       [DEFAULT]
Number of GPUs                                      : 1     [DEFAULT]
Number of channels in image                         : 3     [DEFAULT]
Number of epochs                                    : 150   
Reset layer momentum                                : False [DEFAULT]
Save path                                           : ./    
Test and quit?                                      : 0     [DEFAULT]
Test on one batch at a time?                        : 1     [DEFAULT]
Testing frequency                                   : 15    
Unshare weight matrices in given layers             :       
Whether filp training image                         : True  [DEFAULT]
=========================
Running on CUDA device(s) -2
Current time: Sat Oct 25 21:53:50 2014
Saving checkpoints to ./ConvNet__2014-10-25_21.53.49
=========================
1.1... logprob:  2.041744, 0.740900 (1.849 sec)
1.2... logprob:  1.739498, 0.622900 (1.212 sec)
1.3... logprob:  1.565726, 0.563200 (1.213 sec)
1.4... logprob:  1.529809, 0.549300 (1.210 sec)
1.5... logprob:  1.459259, 0.521900 (1.211 sec)
epoch_cost: 0
2.1... logprob:  1.367006, 0.489600 (1.210 sec)
2.2... logprob:  1.347860, 0.471900 (1.212 sec)
2.3... logprob:  1.253059, 0.442900 (1.211 sec)
2.4... logprob:  1.231878, 0.432200 (1.210 sec)
2.5... logprob:  1.211092, 0.420300 (1.204 sec)
epoch_cost: 0
3.1... logprob:  1.146597, 0.408800 (1.203 sec)
3.2...